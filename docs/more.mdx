# Data Cleaning

Now, let's start cleaning the data.







<CH.Scrollycoding>

## Importing the data

Before we analyze the Titanic dataset, we need to import several Python packages: Pandas, Numpy, sklearn, Matplotlib, etc.

```py titanic.py focus=1:6
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split,GridSearchCV
from sklearn.tree import DecisionTreeRegressor,DecisionTreeClassifier
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline
#import train and test data.
train=pd.read_csv('train.csv')
test=pd.read_csv('test.csv')
name=train.Name
#train.head()
```

---

## The First Glance of the Data

If you are using Jupyter Notebook, you can also include this line to check the dataset.

It will look like this:

| PassengerId | Survived | Pclass | ... |    Sex |  Age | SibSp | ... |
|------------:|---------:|-------:|-----|-------:|-----:|------:|-----|
| 1           | 0        | 3      | ... | male   | 22.0 | 1     | ... |
| 2           | 1        | 1      | ... | female | 38.0 | 1     | ... |
| 3           | 1        | 3      | ... | female | 26.0 | 0     | ... |
| 4           | 1        | 1      | ... | female | 35.0 | 1     | ... |
| 5           | 0        | 3      | ... | male   | 35.0 | 0     | ... |

```py titanic.py focus=12:12

```

---

## What does this data set mean.

The data has been split into two groups:

* training set (train.csv)
* test set(test.csv) 
The training set includes passengers survival status(also know as the ground truth from the titanic tragedy) which along with other features like gender, class, fare and pclass is used to create machine learning model. 

The test set should be used to see how well my model performs on unseen data. The test set does not provide passengers survival status. We are going to use our model to predict passenger survival status. 

Lets describe whats the meaning of the features given the both train & test datasets.

Variable Definition Key.

* Survival
    * 0= No
    * 1= Yes
* pclass (Ticket class)
    * 1=1st
    * 2=2nd
    * 3=3rd
* sex 
* age
* sibsp (# of siblings / spouses aboard the Titanic) 
* parch (# of parents / children aboard the Titanic) 
* tickets 
* fare 
* cabin
* embarked Port of Embarkation.
    * C = Cherbourg,
    * Q = Queenstown,
    * S = Southampton
* pclass: A proxy for socio-economic status (SES) 

This is important to remember and will come in handy for later analysis.
* 1st = Upper
* 2nd = Middle
* 3rd = Lower

---

## Part 1. Cleaning the data.

It looks like this dataset is quite organized, however, before using this dataset for analyzing and visualizing we need to deal with ..

* Different variables
* Null values

### Different variables present in the datasets.

* There are four type of variables
    * Numerical Features: Age, Fare, SibSp and Parch
    * Categorical Features: Sex, Embarked, Survived and Pclass
    * Alphanumeric Features: Ticket and Cabin(Contains both alphabets and the numeric value)
    * Text Features: Name

We really need to tweak these features so we get the desired form of input data.

By using the code in Line 1, we can get the data with NaN.

```py Cleaning.py focus=1:16
Part 1. Cleaning the data
train.isnull().sum()

#PassengerId      0
#Survived         0
#Pclass           0
#Name             0
#Sex              0
#Age            177
#SibSp            0
#Parch            0
#Ticket           0
#Fare             0
#Cabin          687
#Embarked         2
#dtype: int64

Imp=Imputer(missing_values='NaN',strategy='median',axis=1)
new=Imp.fit_transform(train.Age.values.reshape(1,-1))
train['Age2']=new.T
train.drop('Age',axis=1,inplace=True)
train.isnull().sum()

#PassengerId      0
#Survived         0
#Pclass           0
#Name             0
#Sex              0
#Age            177
#SibSp            0
#Parch            0
#Ticket           0
#Fare             0
#Cabin          687
#Embarked         2
#dtype: int64

train.set_index('PassengerId',inplace=True)
## get dummy variables for Column sex and embarked since they are categorical value.
train = pd.get_dummies(train, columns=["Sex"], drop_first=True)
train = pd.get_dummies(train, columns=["Embarked"],drop_first=True)


#Mapping the data.
train['Fare'] = train['Fare'].astype(int)
train.loc[train.Fare<=7.91,'Fare']=0
train.loc[(train.Fare>7.91) &(train.Fare<=14.454),'Fare']=1
train.loc[(train.Fare>14.454)&(train.Fare<=31),'Fare']=2
train.loc[(train.Fare>31),'Fare']=3

train['Age2']=train['Age2'].astype(int)
train.loc[ train['Age2'] <= 16, 'Age2']= 0
train.loc[(train['Age2'] > 16) & (train['Age2'] <= 32), 'Age2'] = 1
train.loc[(train['Age2'] > 32) & (train['Age2'] <= 48), 'Age2'] = 2
train.loc[(train['Age2'] > 48) & (train['Age2'] <= 64), 'Age2'] = 3
train.loc[train['Age2'] > 64, 'Age2'] = 4

# In our data the Ticket and Cabin,Name are the base less,leds to the false prediction so Drop both of them.
train.drop(['Ticket','Cabin','Name'],axis=1,inplace=True)
train.head()
print(type(train.Age2))
```

After we cleaning the NaN value in the **Age** column, the data looks better.
```py Cleaning.py focus=18:36
```

Then, we can continue clean the NaN values in the **Cabin** column.
```py Cleaning.py focus=55:60
```
---

</CH.Scrollycoding>
